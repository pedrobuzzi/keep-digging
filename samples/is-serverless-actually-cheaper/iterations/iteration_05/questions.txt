If senior DevOps/SRE engineers cost $200–400K/year fully loaded, at what server utilization rate does serverless become cheaper *after* labor substitution—and is this threshold more common than critics admit?
The research cites Prime Video as a canonical serverless failure. But what percentage of serverless workloads are high-throughput video processing pipelines? Is this case study representative or systematically misleading?
Critics compare serverless to "optimized alternatives" (Fargate, KEDA, Spot instances). What is the *actual* baseline infrastructure most companies were running before serverless adoption—and does the comparison change when measured against reality rather than the theoretical optimum?
Does the "complexity transfer" critique asymmetrically ignore the complexity of traditional infrastructure? What are the hidden costs of Kubernetes operations, security patching, capacity planning errors, and on-call rotations?
Is the anti-serverless discourse itself subject to selection bias—dominated by engineers at high-scale companies where the economics genuinely fail, while the majority of serverless users at smaller scale have no incentive to write blog posts?
