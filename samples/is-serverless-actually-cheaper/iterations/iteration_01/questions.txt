1. **What are the actual, irreducible cost components of running a computation, and how does serverless vs. traditional infrastructure map onto each one?** (i.e., compute time, memory, storage, networking, cold start overhead, orchestration overhead — which of these does serverless genuinely reduce vs. merely repackage?)
2. **What is the true total cost of ownership (TCO) when you account for the hidden costs serverless introduces — such as vendor lock-in switching costs, observability tooling, inter-service communication overhead, and engineering time spent working around platform constraints?**
3. **At what utilization threshold does the serverless pay-per-invocation model become more expensive than a reserved/dedicated compute model, and what does the empirical data show about where most real workloads fall on this spectrum?**
4. **Is "cheaper" even the right frame — or is serverless trading one type of cost (infrastructure operations) for another type of cost (architectural complexity, debugging difficulty, vendor dependency), and are companies systematically failing to account for the latter?**
5. **What are the incentive structures at play — do cloud providers, consultancies, and developer advocates have financial incentives to promote serverless adoption regardless of whether it's actually cheaper for a given use case?**
